{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QziU_nspppnt"
      },
      "source": [
        "\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGtUNTfkppih"
      },
      "source": [
        "### Install required dependies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpoAZ0e5pmcX",
        "outputId": "7978b4ea-dbfb-4261-c950-5140c7d2fde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m562.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m924.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain \\\n",
        "langchain-core \\\n",
        "langchain-community \\\n",
        "langchain-openai \\\n",
        "langchain-groq \\\n",
        "langchain-google-genai \\\n",
        "langchain-cohere  \\\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM65zga7oxJG"
      },
      "source": [
        "### Environment checkup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6GaDzRgrTbt",
        "outputId": "969244c9-6b09-4f1f-f325-dde1c6dad923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Environment check\n",
            "============================================================\n",
            "Gemini key Found. AIzaSyDSmx*****\n",
            "Open Router key Found. sk-or-v1-5*****\n",
            "Groq key Found. gsk_zQKvLC*****\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print('='*60)\n",
        "print(f'Environment check')\n",
        "print('='*60)\n",
        "from google.colab import userdata\n",
        "gemini_key = userdata.get('GEMINI_API_KEY')[:10]\n",
        "if gemini_key:\n",
        "  print(f\"Gemini key Found. {gemini_key}*****\")\n",
        "openrouter_key = userdata.get('OPENROUTER_API_KEY')[:10]\n",
        "if openrouter_key:\n",
        "  print(f\"Open Router key Found. {openrouter_key}*****\")\n",
        "groq_key = userdata.get('GROQ_API_KEY')[:10]\n",
        "if groq_key:\n",
        "  print(f\"Groq key Found. {groq_key}*****\")\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEDTR9qtfW08",
        "outputId": "e231b948-e282-49b2-dcfa-8410912fee85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set Gemini API key in the enviornment variables\n",
            "Set Groq API key in the enviornment variables\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "print(\"Set Gemini API key in the enviornment variables\")\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "print(\"Set Groq API key in the enviornment variables\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlNJaF7kr2pd",
        "outputId": "51a051db-dc23-45c7-c1d5-b7cd436edd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Environment check\n",
            "============================================================\n",
            "Python Version:3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Device:CPU\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print('='*60)\n",
        "print(f'Environment check')\n",
        "print('='*60)\n",
        "print(f'Python Version:{sys.version}')\n",
        "print(f'Device:{'Cuda' if torch.cuda.is_available() else 'CPU'}')\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg8f9xvEsPrs"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "  data_path: str = '/content/data'\n",
        "  out_dir: str = '/content/output'\n",
        "  chunk_size: int = 1500\n",
        "  chunk_overlap: int = 200\n",
        "  qa_per_chunk: int = 10\n",
        "  train_split: float = 0.8\n",
        "\n",
        "  base_provider: str = \"groq\"\n",
        "  provider: str = \"groq\"\n",
        "  model: str = \"llama-3.1-8b-instant\"\n",
        "\n",
        "  temperature: float = 0.2\n",
        "  max_tokens: int =384\n",
        "  request_timeout: int =60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cva4RGKur4X",
        "outputId": "cee7075f-fdbf-4460-8d48-856e28eaeb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Config(data_path='/content/data',\n",
            "       out_dir='/content/output',\n",
            "       chunk_size=1500,\n",
            "       chunk_overlap=200,\n",
            "       qa_per_chunk=10,\n",
            "       train_split=0.8,\n",
            "       base_provider='groq',\n",
            "       provider='groq',\n",
            "       model='llama-3.1-8b-instant',\n",
            "       temperature=0.2,\n",
            "       max_tokens=384,\n",
            "       request_timeout=60)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "config = Config()\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "pprint(config)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmGjgEdznlt"
      },
      "source": [
        "### Load PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFv3xd7Hzh57",
        "outputId": "b286df81-7cd7-4edd-fce8-1e1a0f21b750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loaded 142 document pages\n",
            " Total characters: 639,466\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    DirectoryLoader,\n",
        "    TextLoader\n",
        ")\n",
        "pdf_dir = Path(config.data_path)\n",
        "pdf_files=  list(pdf_dir.glob('*.pdf'))\n",
        "if len(pdf_files) == 0:\n",
        "    print(f\" No PDFs found at {pdf_dir}\")\n",
        "else:\n",
        "  documents = []\n",
        "  for pdf_path in pdf_files:\n",
        "    loader = PyPDFLoader(str(pdf_path))\n",
        "\n",
        "    doc = loader.load()\n",
        "    documents.extend(doc)\n",
        "    print(f\" Loaded {len(documents)} document pages\")\n",
        "    print(f\" Total characters: {sum(len(d.page_content) for d in documents):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quPEgheBG9MH"
      },
      "source": [
        "### Simple Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0V3YMkSHFIy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_whitespace(text: str) -> str:\n",
        "    \"\"\"\n",
        "    - Normalize newlines\n",
        "    - Collapse multiple spaces\n",
        "    - Collapse excessive blank lines\n",
        "    \"\"\"\n",
        "    # Normalize newlines\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    # Remove trailing/leading whitespace per line\n",
        "    lines = [line.strip() for line in text.split(\"\\n\")]\n",
        "\n",
        "    # Collapse multiple spaces inside lines\n",
        "    lines = [re.sub(r\"\\s{2,}\", \" \", line) for line in lines]\n",
        "\n",
        "    # Rebuild text\n",
        "    text = \"\\n\".join(lines)\n",
        "\n",
        "    # Collapse 3+ newlines into max 2\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wu5eVufHNK3",
        "outputId": "6cf3cecf-1c70-4796-bb34-456804dfd6ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned the files for chunking\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "cleaned_documents = []\n",
        "\n",
        "for doc in documents:\n",
        "    cleaned_text = clean_whitespace(doc.page_content)\n",
        "    cleaned_documents.append(\n",
        "        Document(\n",
        "            page_content=cleaned_text,\n",
        "            metadata=doc.metadata\n",
        "        )\n",
        "    )\n",
        "print(\"Cleaned the files for chunking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkPw3nPd5a30"
      },
      "source": [
        "### Fixed size chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2AClhza4ncO",
        "outputId": "fda4551f-40e2-4c91-c6d5-7038e1909881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No of chunks: 532\n",
            "\n",
            "Example chunk:\n",
            "  Length: 579 chars\n",
            "  Content: Uber’s Mission\n",
            "We reimagine the way the world moves for the better\n",
            "We are Uber. The go-getters. The kind of people who are relentless about our\n",
            "missio...\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "splitter = CharacterTextSplitter(\n",
        "    chunk_size=config.chunk_size,\n",
        "    chunk_overlap=config.chunk_overlap,\n",
        "    separator=\" \"\n",
        ")\n",
        "chunks = splitter.split_documents(cleaned_documents)\n",
        "for i, d in enumerate(chunks):\n",
        "    d.metadata[\"chunk_id\"] = i\n",
        "print(f\"No of chunks: {len(chunks)}\")\n",
        "print(f\"\\nExample chunk:\")\n",
        "print(f\"  Length: {len(chunks[1].page_content)} chars\")\n",
        "print(f\"  Content: {chunks[1].page_content[:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u12WtYG0v60d"
      },
      "outputs": [],
      "source": [
        "def get_llm(config):\n",
        "    \"\"\"\n",
        "    Return a LangChain-compatible chat model based on config.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration dictionary with llm_provider, llm_model, etc.\n",
        "\n",
        "    Returns:\n",
        "        LangChain chat model instance\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If provider is unknown\n",
        "    \"\"\"\n",
        "    base_provider = config.base_provider\n",
        "\n",
        "    if base_provider == \"openrouter\":\n",
        "        openrouter_provider = config.provider\n",
        "        openrouter_model = config.model\n",
        "        model_name = f\"{openrouter_provider}/{openrouter_model}\"\n",
        "\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        return ChatOpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=userdata.get(\"OPENROUTER_API_KEY\"),\n",
        "            model=model_name,\n",
        "            temperature=config.temperature,\n",
        "            max_tokens=config.max_tokens,\n",
        "            timeout=config.request_timeout,\n",
        "        )\n",
        "\n",
        "    elif base_provider == \"gemini\":\n",
        "        model_name = config.model\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        return ChatGoogleGenerativeAI(\n",
        "            model=model_name,\n",
        "            temperature=config.temperature,\n",
        "            max_tokens=config.max_tokens,\n",
        "        )\n",
        "    elif base_provider == \"groq\" :\n",
        "        model_name = config.model\n",
        "        from langchain_groq import ChatGroq\n",
        "        return ChatGroq(\n",
        "            model = model_name,\n",
        "            temperature= config.temperature\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown llm_provider: {provider}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26TwJUvBRTjR"
      },
      "source": [
        "### LLM sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm4totupLoTB",
        "outputId": "abbd9117-1e9b-4de8-9694-0be6dbb228dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing LLM API connection...\n",
            "{'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.0 (Macintosh)', 'creationdate': '2025-03-20T10:48:37+05:30', 'moddate': '2025-03-22T09:33:29-04:00', 'trapped': '/False', 'source': '/content/data/annual_report.pdf', 'total_pages': 142, 'page': 24, 'page_label': 'y', 'chunk_id': 100}\n",
            "Chunk ID: 100\n",
            "Page: 24\n",
            "we are not successful in penetrating suburban and rural areas, or if we are unable to operate in certain key metropolitan ar eas in the\n",
            "future, our ability to serve what we consider to be our total addressable market would be limited, and our business, financial condition,\n",
            "and operating results would suffer.\n",
            "In 2024, we generated 15% of our Mobility Gross Bookings from tr ips that either started or were completed at an airport. As a\n",
            "result of this concentration, our operating results are suscept\n",
            "************************************************************\n",
            "Output from the Model\n",
            "************************************************************\n",
            "[\n",
            "  {\n",
            "    \"category\": \"HardFact\",\n",
            "    \"question\": \"What percentage of Mobility Gross Bookings in 2024 were generated from trips that either started or were completed at an airport?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"HardFact\",\n",
            "    \"question\": \"What is the percentage of Mobility Gross Bookings from airport trips?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"HardFact\",\n",
            "    \"question\": \"In what year did travel behavior change and airline travel slow down, reducing demand for Mobility to and from airports?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"HardFact\",\n",
            "    \"question\": \"What is the percentage of Mobility Gross Bookings from airport trips in 2024?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Strategy\",\n",
            "    \"question\": \"What would happen to the company's business, financial condition, and operating results if it is unable to operate in certain key metropolitan areas?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Strategy\",\n",
            "    \"question\": \"What risks does the company face due to its concentration of Mobility Gross Bookings from airport trips?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Strategy\",\n",
            "    \"question\": \"How would sustained declines in air travel affect the company's Mobility Gross Bookings from airport trips?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Style\",\n",
            "    \"question\": \"What tone is conveyed in the statement that some Drivers continue to provide Mobility services despite lacking the requisite permits?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Style\",\n",
            "    \"question\": \"What is the implication of the statement that certain airports have banned ridesharing operations altogether?\"\n",
            "  },\n",
            "  {\n",
            "    \"category\": \"Style\",\n",
            "    \"question\": \"What is the overall tone of the passage regarding the company's ability to serve its total addressable market?\"\n",
            "  }\n",
            "]\n",
            "************************************************************\n"
          ]
        }
      ],
      "source": [
        "llm_a = get_llm(config)\n",
        "\n",
        "print(\"\\nTesting LLM API connection...\")\n",
        "try:\n",
        "  test_chunk = chunks[100]\n",
        "  print(test_chunk.metadata)\n",
        "  print(\"Chunk ID:\", test_chunk.metadata.get(\"chunk_id\"))\n",
        "  print(\"Page:\", test_chunk.metadata.get(\"page_start\", test_chunk.metadata.get(\"page\")))\n",
        "  print(test_chunk.page_content[:500])\n",
        "\n",
        "  test_prompt_A =f\"\"\"\n",
        "  ROLE:\n",
        "  You are a senior financial analyst creating training data from an annual report.\n",
        "\n",
        "  TASK:\n",
        "  Generate questions that can be answered strictly using the provided context.\n",
        "\n",
        "  CONTEXT:\n",
        "  {test_chunk.page_content}\n",
        "\n",
        "  CONSTRAINTS:\n",
        "  - Use ONLY the information in the CONTEXT. Do not use outside knowledge.\n",
        "  - Generate EXACTLY 10 questions.\n",
        "  - Enforce the following category distribution exactly:\n",
        "    • HardFact: 4 questions (exact figures, dates, entities, or explicitly stated facts)\n",
        "    • Strategy: 3 questions (summaries, drivers, risks, or implications based on the context)\n",
        "    • Style: 3 questions (stylistic or creative outputs such as memos, briefs, or tone-based summaries)\n",
        "  - Questions must be specific, non-duplicative, and clearly answerable from the CONTEXT.\n",
        "  - Do NOT include answers.\n",
        "  - Do NOT include explanations, markdown, or any extra text.\n",
        "\n",
        "  OUTPUT FORMAT:\n",
        "  Return a valid JSON array of exactly 10 objects.\n",
        "  Each object MUST have the following fields:\n",
        "  - \"category\": one of [\"HardFact\", \"Strategy\", \"Style\"]\n",
        "  - \"question\": string\n",
        "\n",
        "  Example format:\n",
        "  [\n",
        "    dictionary as \"category\": \"HardFact\", \"question\": \"...\",\n",
        "    ...\n",
        "  ]\n",
        "  \"\"\"\n",
        "\n",
        "  response = llm_a.invoke(test_prompt_A)\n",
        "  raw_output = response.content\n",
        "  print(\"*\"*60)\n",
        "  print(\"Output from the Model\")\n",
        "  print(\"*\"*60)\n",
        "  print(raw_output)\n",
        "  print(\"*\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"LLM API test failed: {e}\")\n",
        "    print(\"Please check your .env file and API key configuration.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xYQ9bb6SSfG"
      },
      "source": [
        "### Question Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj25ilY1SVgU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "categories = {\"HardFact\", \"Strategy\", \"Style\"}\n",
        "\n",
        "def parse_json_array(text: str):\n",
        "    \"\"\"Parse JSON array from model output (assumes prompt asked JSON-only).\"\"\"\n",
        "    return json.loads(text)\n",
        "\n",
        "def validate_questions(q_list):\n",
        "    \"\"\"Return (ok: bool, error: str).\"\"\"\n",
        "    if not isinstance(q_list, list):\n",
        "        return False, \"Not a list\"\n",
        "\n",
        "    if len(q_list) != 10:\n",
        "        return False, f\"Expected 10 questions, got {len(q_list)}\"\n",
        "\n",
        "    for i, item in enumerate(q_list):\n",
        "        if not isinstance(item, dict):\n",
        "            return False, f\"Item {i} not an object\"\n",
        "        if \"category\" not in item or \"question\" not in item:\n",
        "            return False, f\"Item {i} missing keys\"\n",
        "        if item[\"category\"] not in categories:\n",
        "            return False, f\"Item {i} has invalid category: {item['category']}\"\n",
        "        if not isinstance(item[\"question\"], str) or not item[\"question\"].strip():\n",
        "            return False, f\"Item {i} empty question\"\n",
        "\n",
        "    counts = Counter([x[\"category\"] for x in q_list])\n",
        "    if counts[\"HardFact\"] != 4 or counts[\"Strategy\"] != 3 or counts[\"Style\"] != 3:\n",
        "        return False, f\"Bad category distribution: {dict(counts)}\"\n",
        "\n",
        "    return True, \"ok\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHn3aYLOTPFV"
      },
      "outputs": [],
      "source": [
        "def make_question_prompt(chunk_text: str) -> str:\n",
        "    return f\"\"\"\n",
        "ROLE:\n",
        "You are a senior financial analyst creating training data from an annual report.\n",
        "\n",
        "TASK:\n",
        "Generate questions that can be answered strictly using the provided context.\n",
        "\n",
        "CONTEXT:\n",
        "{chunk_text}\n",
        "\n",
        "CONSTRAINTS:\n",
        "- Use ONLY the information in the CONTEXT.\n",
        "- Generate EXACTLY 10 questions.\n",
        "- Category distribution must be:\n",
        "  • HardFact: 4\n",
        "  • Strategy: 3\n",
        "  • Style: 3\n",
        "- Questions must be specific and non-duplicative.\n",
        "- Do NOT include answers.\n",
        "- Output MUST be valid JSON only. No extra text.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Return a JSON array of exactly 10 objects:\n",
        "[\n",
        "  {{\"category\": \"HardFact\", \"question\": \"...\"}},\n",
        "  ...\n",
        "]\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i-R0psDTmHS",
        "outputId": "feb445af-8b0c-4862-f0c5-ffcfae9a3a0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 532/532 [58:52<00:00,  6.64s/it]\n"
          ]
        }
      ],
      "source": [
        "import os, time\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "CHECKPOINT_EVERY = 10\n",
        "CHECKPOINT_PATH = \"/content/output/questions_checkpoint.json\"\n",
        "\n",
        "questions_by_chunk = {}\n",
        "fail_log = []\n",
        "\n",
        "# optional: resume from checkpoint\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    with open(CHECKPOINT_PATH, \"r\") as f:\n",
        "        questions_by_chunk = json.load(f)\n",
        "    # keys might be strings after JSON load\n",
        "    questions_by_chunk = {int(k): v for k, v in questions_by_chunk.items()}\n",
        "    print(f\"Resumed from checkpoint: {len(questions_by_chunk)} chunks loaded\")\n",
        "\n",
        "for d in tqdm(chunks):\n",
        "    chunk_id = d.metadata.get(\"chunk_id\")\n",
        "    if chunk_id is None:\n",
        "        continue\n",
        "\n",
        "    # skip if already done (resume support)\n",
        "    if chunk_id in questions_by_chunk:\n",
        "        continue\n",
        "\n",
        "    prompt = make_question_prompt(d.page_content)\n",
        "\n",
        "    ok = False\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = llm_a.invoke(prompt).content\n",
        "            q_list = parse_json_array(resp)\n",
        "            valid, msg = validate_questions(q_list)\n",
        "            if valid:\n",
        "                questions_by_chunk[chunk_id] = q_list\n",
        "                ok = True\n",
        "                break\n",
        "            else:\n",
        "                last_err = msg\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "\n",
        "        # brief backoff (helps rate limits)\n",
        "        time.sleep(1.0 * attempt)\n",
        "\n",
        "    if not ok:\n",
        "        fail_log.append({\"chunk_id\": chunk_id, \"error\": last_err})\n",
        "        # you can decide to continue or raise\n",
        "        continue\n",
        "\n",
        "    # checkpoint periodically\n",
        "    if len(questions_by_chunk) % CHECKPOINT_EVERY == 0:\n",
        "        with open(CHECKPOINT_PATH, \"w\") as f:\n",
        "            json.dump(questions_by_chunk, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aIlcF4NoNb"
      },
      "source": [
        "#### LLM B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNqWN4mNdAEd",
        "outputId": "178673f8-60e7-4364-8ed2-14a9125db593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Config(data_path='/content/data',\n",
            "       out_dir='/content/output',\n",
            "       chunk_size=1500,\n",
            "       chunk_overlap=200,\n",
            "       qa_per_chunk=10,\n",
            "       train_split=0.8,\n",
            "       base_provider='groq',\n",
            "       provider='groq',\n",
            "       model='meta-llama/llama-4-scout-17b-16e-instruct',\n",
            "       temperature=0.2,\n",
            "       max_tokens=384,\n",
            "       request_timeout=60)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "config_b = Config(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "pprint(config_b)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP3DTVELdcnt",
        "outputId": "3d1419a1-497f-4dd3-d85c-f1369be90cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing LLM API connection...\n",
            "{'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.0 (Macintosh)', 'creationdate': '2025-03-20T10:48:37+05:30', 'moddate': '2025-03-22T09:33:29-04:00', 'trapped': '/False', 'source': '/content/data/annual_report.pdf', 'total_pages': 142, 'page': 0, 'page_label': 'a', 'chunk_id': 0}\n",
            "Chunk ID: 0\n",
            "Page: 0\n",
            "On Our Way\n",
            "2024 ANNUAL REPORT\n",
            "************************************************************\n",
            "Output from the Model\n",
            "************************************************************\n",
            "[\n",
            "  {\n",
            "    \"category\": \"HardFact\",\n",
            "    \"question\": \"What is the address of the company's headquarters?\",\n",
            "    \"answer\": \"Not available in the provided context.\"\n",
            "  }\n",
            "]\n",
            "************************************************************\n"
          ]
        }
      ],
      "source": [
        "llm_b = get_llm(config_b)\n",
        "print(\"\\nTesting LLM API connection...\")\n",
        "try:\n",
        "  test_chunk = chunks[0]\n",
        "  test_question = q_list[0]\n",
        "  print(test_chunk.metadata)\n",
        "  print(\"Chunk ID:\", test_chunk.metadata.get(\"chunk_id\"))\n",
        "  print(\"Page:\", test_chunk.metadata.get(\"page_start\", test_chunk.metadata.get(\"page\")))\n",
        "  print(test_chunk.page_content[:500])\n",
        "\n",
        "  test_prompt_b =f\"\"\"\n",
        "  ROLE:\n",
        "You are a financial analyst answering questions using an annual report excerpt.\n",
        "\n",
        "TASK:\n",
        "Answer the provided questions using only the given context.\n",
        "\n",
        "CONTEXT:\n",
        "{test_chunk}\n",
        "\n",
        "QUESTIONS:\n",
        "{test_question}\n",
        "\n",
        "CONSTRAINTS:\n",
        "- Use ONLY the information in the CONTEXT.\n",
        "- Do NOT use any external knowledge or assumptions.\n",
        "- If the answer is not explicitly supported by the CONTEXT, respond exactly with:\n",
        "  \"Not available in the provided context.\"\n",
        "- Keep answers concise and factual.\n",
        "- For numeric answers, copy figures exactly as they appear in the CONTEXT.\n",
        "- Output MUST be valid JSON only. No extra text.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Return a JSON array with the SAME length and order as the input QUESTIONS:\n",
        "[\n",
        "  {{\"category\": \"...\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "  ...\n",
        "]\n",
        "  \"\"\"\n",
        "\n",
        "  response = llm_a.invoke(test_prompt_b)\n",
        "  raw_output = response.content\n",
        "  print(\"*\"*60)\n",
        "  print(\"Output from the Model\")\n",
        "  print(\"*\"*60)\n",
        "  print(raw_output)\n",
        "  print(\"*\"*60)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"LLM API test failed: {e}\")\n",
        "    print(\"Please check your .env file and API key configuration.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjK47te5Le0Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def make_answer_prompt(chunk_text: str, questions_json: str) -> str:\n",
        "    return f\"\"\"\n",
        "ROLE:\n",
        "You are a financial analyst answering questions using an annual report excerpt.\n",
        "\n",
        "TASK:\n",
        "Answer the provided questions using only the given context.\n",
        "\n",
        "CONTEXT:\n",
        "{chunk_text}\n",
        "\n",
        "QUESTIONS:\n",
        "{questions_json}\n",
        "\n",
        "CONSTRAINTS:\n",
        "- Use ONLY the information in the CONTEXT.\n",
        "- Do NOT use any external knowledge or assumptions.\n",
        "- If the answer is not explicitly supported by the CONTEXT, respond exactly with:\n",
        "  \"Not available in the provided context.\"\n",
        "- Keep answers concise and factual.\n",
        "- For numeric answers, copy figures exactly as they appear in the CONTEXT.\n",
        "- Output MUST be valid JSON only. No extra text.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Return a JSON array with the SAME length and order as the input QUESTIONS:\n",
        "[\n",
        "  {{\"category\": \"...\", \"question\": \"...\", \"answer\": \"...\"}},\n",
        "  ...\n",
        "]\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3Ru6F7GgPiU"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "categories = {\"HardFact\", \"Strategy\", \"Style\"}\n",
        "\n",
        "def parse_json_array(text: str):\n",
        "    return json.loads(text)\n",
        "\n",
        "def validate_answers(ans_list):\n",
        "    if not isinstance(ans_list, list):\n",
        "        return False, \"Not a list\"\n",
        "    if len(ans_list) != 10:\n",
        "        return False, f\"Expected 10 answers, got {len(ans_list)}\"\n",
        "\n",
        "    for i, item in enumerate(ans_list):\n",
        "        if not isinstance(item, dict):\n",
        "            return False, f\"Item {i} not an object\"\n",
        "        for key in (\"category\", \"question\", \"answer\"):\n",
        "            if key not in item:\n",
        "                return False, f\"Item {i} missing key: {key}\"\n",
        "        if item[\"category\"] not in categories:\n",
        "            return False, f\"Item {i} invalid category: {item['category']}\"\n",
        "        if not isinstance(item[\"question\"], str) or not item[\"question\"].strip():\n",
        "            return False, f\"Item {i} empty question\"\n",
        "        if not isinstance(item[\"answer\"], str) or not item[\"answer\"].strip():\n",
        "            return False, f\"Item {i} empty answer\"\n",
        "\n",
        "    # Optional: ensure category distribution preserved (should match questions)\n",
        "    counts = Counter([x[\"category\"] for x in ans_list])\n",
        "    if counts[\"HardFact\"] != 4 or counts[\"Strategy\"] != 3 or counts[\"Style\"] != 3:\n",
        "        return False, f\"Bad category distribution: {dict(counts)}\"\n",
        "\n",
        "    return True, \"ok\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kki8bCQUgWNY",
        "outputId": "7635fbd6-b776-42d3-cbb3-1f04e166fbbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumed answers checkpoint: 156 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 529/529 [20:29<00:00,  2.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers generated for chunks: 520\n",
            "Failures: 9\n",
            "Sample failure: {'chunk_id': 317, 'error': 'Item 0 missing key: answer'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, time, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "CHECKPOINT_EVERY = 10\n",
        "\n",
        "ANS_CHECKPOINT_PATH = \"/content/output/answers_checkpoint.json\"\n",
        "FAIL_LOG_PATH = \"/content/output/answer_fail_log.json\"\n",
        "\n",
        "# We'll store answers by chunk_id, and also flatten into Q/A records later\n",
        "answers_by_chunk = {}\n",
        "fail_log = []\n",
        "\n",
        "# Resume if checkpoint exists\n",
        "if os.path.exists(ANS_CHECKPOINT_PATH):\n",
        "    with open(ANS_CHECKPOINT_PATH, \"r\") as f:\n",
        "        answers_by_chunk = json.load(f)\n",
        "    answers_by_chunk = {int(k): v for k, v in answers_by_chunk.items()}\n",
        "    print(f\"Resumed answers checkpoint: {len(answers_by_chunk)} chunks\")\n",
        "\n",
        "# Helpful: build a quick lookup from chunk_id -> chunk Document\n",
        "chunk_map = {d.metadata.get(\"chunk_id\"): d for d in chunks}\n",
        "\n",
        "for chunk_id, q_list in tqdm(questions_by_chunk.items()):\n",
        "    if chunk_id in answers_by_chunk:\n",
        "        continue\n",
        "\n",
        "    d = chunk_map.get(chunk_id)\n",
        "    if d is None:\n",
        "        fail_log.append({\"chunk_id\": chunk_id, \"error\": \"chunk_id not found in chunks\"})\n",
        "        continue\n",
        "\n",
        "    questions_json = json.dumps(q_list, ensure_ascii=False)\n",
        "    prompt = make_answer_prompt(d.page_content, questions_json)\n",
        "\n",
        "    ok = False\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = llm_b.invoke(prompt).content\n",
        "            ans_list = parse_json_array(resp)\n",
        "            valid, msg = validate_answers(ans_list)\n",
        "            if valid:\n",
        "                answers_by_chunk[chunk_id] = ans_list\n",
        "                ok = True\n",
        "                break\n",
        "            else:\n",
        "                last_err = msg\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "\n",
        "        time.sleep(1.0 * attempt)  # backoff\n",
        "\n",
        "    if not ok:\n",
        "        fail_log.append({\"chunk_id\": chunk_id, \"error\": last_err})\n",
        "        continue\n",
        "\n",
        "    # checkpoint periodically\n",
        "    if len(answers_by_chunk) % CHECKPOINT_EVERY == 0:\n",
        "        with open(ANS_CHECKPOINT_PATH, \"w\") as f:\n",
        "            json.dump(answers_by_chunk, f, ensure_ascii=False)\n",
        "        with open(FAIL_LOG_PATH, \"w\") as f:\n",
        "            json.dump(fail_log, f, ensure_ascii=False)\n",
        "\n",
        "# Final save\n",
        "with open(ANS_CHECKPOINT_PATH, \"w\") as f:\n",
        "    json.dump(answers_by_chunk, f, ensure_ascii=False)\n",
        "with open(FAIL_LOG_PATH, \"w\") as f:\n",
        "    json.dump(fail_log, f, ensure_ascii=False)\n",
        "\n",
        "print(\"Answers generated for chunks:\", len(answers_by_chunk))\n",
        "print(\"Failures:\", len(fail_log))\n",
        "if fail_log:\n",
        "    print(\"Sample failure:\", fail_log[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJCbBhY6k7CF",
        "outputId": "16184518-b82b-4936-e75f-d65c9469aff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 5200\n",
            "Missing chunks: 0\n",
            "Sample record:\n",
            " {'instruction': 'Answer the question using ONLY the information provided in the context. If the answer is not present in the context, reply exactly: \"Not available in the provided context.\"', 'input': 'Context:\\nOn Our Way\\n2024 ANNUAL REPORT\\n\\nQuestion:\\nWhat is the total revenue for the year 2024?', 'output': '$1,234,567', 'category': 'HardFact'}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import datetime\n",
        "\n",
        "# fixed instruction for all examples (keep identical for SFT consistency)\n",
        "INSTRUCTION = (\n",
        "    'Answer the question using ONLY the information provided in the context. '\n",
        "    'If the answer is not present in the context, reply exactly: '\n",
        "    '\"Not available in the provided context.\"'\n",
        ")\n",
        "\n",
        "# chunk_id -> chunk doc lookup\n",
        "chunk_map = {d.metadata.get(\"chunk_id\"): d for d in chunks}\n",
        "\n",
        "records = []\n",
        "missing_chunks = 0\n",
        "\n",
        "for chunk_id, ans_list in answers_by_chunk.items():\n",
        "    chunk_doc = chunk_map.get(chunk_id)\n",
        "    if chunk_doc is None:\n",
        "        missing_chunks += 1\n",
        "        continue\n",
        "\n",
        "    context = chunk_doc.page_content.strip()\n",
        "\n",
        "    for qa in ans_list:\n",
        "        q = qa[\"question\"].strip()\n",
        "        a = qa[\"answer\"].strip()\n",
        "        cat = qa.get(\"category\", \"Unknown\")\n",
        "\n",
        "        rec = {\n",
        "            \"instruction\": INSTRUCTION,\n",
        "            \"input\": f\"Context:\\n{context}\\n\\nQuestion:\\n{q}\",\n",
        "            \"output\": a,\n",
        "            \"category\": cat,\n",
        "        }\n",
        "        records.append(rec)\n",
        "\n",
        "print(\"Total records:\", len(records))\n",
        "print(\"Missing chunks:\", missing_chunks)\n",
        "print(\"Sample record:\\n\", records[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS6vBoxzmBPj",
        "outputId": "8286e699-6480-41ab-fd16-8ab652a0ec3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category counts: {'HardFact': 2080, 'Strategy': 1560, 'Style': 1560}\n",
            "Abstain count: 990 (19.04%)\n",
            "Train: 4160\n",
            "Golden: 1040\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "cat_counts = Counter(r[\"category\"] for r in records)\n",
        "abstain = sum(1 for r in records if r[\"output\"] == \"Not available in the provided context.\")\n",
        "\n",
        "print(\"Category counts:\", dict(cat_counts))\n",
        "print(\"Abstain count:\", abstain, f\"({abstain/len(records)*100:.2f}%)\")\n",
        "\n",
        "SEED = 42\n",
        "rng = random.Random(SEED)\n",
        "rng.shuffle(records)\n",
        "\n",
        "split_idx = int(0.8 * len(records))\n",
        "train_records = records[:split_idx]\n",
        "golden_records = records[split_idx:]\n",
        "\n",
        "print(\"Train:\", len(train_records))\n",
        "print(\"Golden:\", len(golden_records))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmlypRkjmoc0",
        "outputId": "56e002f7-c46f-4dff-89bc-2672fe872ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/data/processed/train.jsonl\n",
            "Saved: /content/data/processed/test_set.jsonl\n"
          ]
        }
      ],
      "source": [
        "import os, json\n",
        "\n",
        "OUT_DIR = \"/content/data/processed\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "train_path = os.path.join(OUT_DIR, \"train.jsonl\")\n",
        "test_path = os.path.join(OUT_DIR, \"test_set.jsonl\")\n",
        "\n",
        "def write_jsonl(path, rows):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in rows:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "write_jsonl(train_path, train_records)\n",
        "write_jsonl(test_path, golden_records)\n",
        "\n",
        "print(\"Saved:\", train_path)\n",
        "print(\"Saved:\", test_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
